{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import itertools\n",
    "import random\n",
    "from model import EncoderRNN,LuongAttnDecoderRNN \n",
    "from decoder import GreedySearchDecoder,BeamSearchDecoder\n",
    "from train import train, trainIters \n",
    "from evals import evaluateInput\n",
    "import config\n",
    "from corpus import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preparing training data...\n",
      "Read 72922 sentence pairs\n",
      "Trimmed to 21265 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 10178\n",
      "keep_words 7675 / 10174 = 0.7544\n",
      "\n",
      "pairs:\n",
      "[5, 0, 'hey have you seen the inception ? ', 'no i have not but have heard of it . what is it about']\n",
      "[5, 0, 'hey have you seen the inception ? no i have not but have heard of it . what is it about ', 'it s about extractors that perform experiments using military technology o n people to retrieve info about their targets .']\n",
      "[5, 0, 'hey have you seen the inception ? no i have not but have heard of it . what is it about it s about extractors that perform experiments using military technology o n people to retrieve info about their targets . ', 'sounds interesting do you know which actors are in it ?']\n",
      "[5, 0, 'hey have you seen the inception ? no i have not but have heard of it . what is it about it s about extractors that perform experiments using military technology o n people to retrieve info about their targets . sounds interesting do you know which actors are in it ? i have not watched it either or seen a preview . UNK it s scifi so it might be good . ugh leonardo dicaprio is the main character he plays as don cobb ', 'oh okay yeah i am not a big scifi fan but there are a few movies i still enjoy in that genre .']\n",
      "[28, 0, 'hello . ', 'hello']\n",
      "[28, 0, 'hello . hello what movie do you like ? ', 'well i like comedy .']\n",
      "[28, 0, 'hello . hello what movie do you like ? well i like comedy . ', 'which movie ?']\n",
      "[28, 0, 'hello . hello what movie do you like ? well i like comedy . which movie ? ', 'i should be asking you this question .']\n",
      "[28, 0, 'hello . hello what movie do you like ? well i like comedy . which movie ? i should be asking you this question . what is the name of the movie ? ', 'i like faster furious']\n",
      "[28, 0, 'hello . hello what movie do you like ? well i like comedy . which movie ? i should be asking you this question . what is the name of the movie ? i like faster furious ', 'is this the movie that you have the doment of ?']\n",
      "[28, 1, 'hello . hello what movie do you like ? well i like comedy . which movie ? i should be asking you this question . what is the name of the movie ? i like faster furious is this the movie that you have the doment of ? ', 'yes some']\n",
      "[28, 1, 'hello . hello what movie do you like ? well i like comedy . which movie ? i should be asking you this question . what is the name of the movie ? i like faster furious is this the movie that you have the doment of ? yes some ', 'fast and furious is in your document ?']\n",
      "[28, 1, 'hello . hello what movie do you like ? well i like comedy . which movie ? i should be asking you this question . what is the name of the movie ? i like faster furious is this the movie that you have the doment of ? yes some fast and furious is in your document ? ', 'yes']\n",
      "[28, 1, 'hello . hello what movie do you like ? well i like comedy . which movie ? i should be asking you this question . what is the name of the movie ? i like faster furious is this the movie that you have the doment of ? yes some fast and furious is in your document ? yes ', 'ok .']\n",
      "[28, 1, 'hello . hello what movie do you like ? well i like comedy . which movie ? i should be asking you this question . what is the name of the movie ? i like faster furious is this the movie that you have the doment of ? yes some fast and furious is in your document ? yes ok . yes i like that movie a lot . ', 'what is in your document ?']\n",
      "[28, 1, 'hello . hello what movie do you like ? well i like comedy . which movie ? i should be asking you this question . what is the name of the movie ? i like faster furious is this the movie that you have the doment of ? yes some fast and furious is in your document ? yes ok . yes i like that movie a lot . what is in your document ? ', 'i do not have one .']\n",
      "[28, 2, 'hello . hello what movie do you like ? well i like comedy . which movie ? i should be asking you this question . what is the name of the movie ? i like faster furious is this the movie that you have the doment of ? yes some fast and furious is in your document ? yes ok . yes i like that movie a lot . what is in your document ? i do not have one . ', 'what scene did you like ?']\n",
      "[28, 2, 'hello . hello what movie do you like ? well i like comedy . which movie ? i should be asking you this question . what is the name of the movie ? i like faster furious is this the movie that you have the doment of ? yes some fast and furious is in your document ? yes ok . yes i like that movie a lot . what is in your document ? i do not have one . what scene did you like ? ', 'i really would like to watch that movie . which part it is ?']\n",
      "[25, 0, 'hello are you able to see the movie information ? ', 'hello ! and yes . you should watch a movie called the great gatsby .']\n",
      "[25, 0, 'hello are you able to see the movie information ? hello ! and yes . you should watch a movie called the great gatsby . ', 'oh is it a good movie ?']\n",
      "[25, 0, 'hello are you able to see the movie information ? hello ! and yes . you should watch a movie called the great gatsby . oh is it a good movie ? ', 'it is ! it has a huge production budget of almost 200 million so it s likely very pretty']\n",
      "[22, 0, 'hello happy friday ! ', 'hello there !']\n",
      "[22, 0, 'hello happy friday ! hello there ! ', 'have you seen this movie ?']\n",
      "[22, 0, 'hello happy friday ! hello there ! have you seen this movie ? ', 'what is the name of the movie ?']\n",
      "[22, 0, 'hello happy friday ! hello there ! have you seen this movie ? what is the name of the movie ? ', 'maleficent']\n",
      "[22, 0, 'hello happy friday ! hello there ! have you seen this movie ? what is the name of the movie ? maleficent with angelina jolie ', 'no i ve heard of it but never watched it']\n",
      "[22, 0, 'hello happy friday ! hello there ! have you seen this movie ? what is the name of the movie ? maleficent with angelina jolie no i ve heard of it but never watched it what is it about ? ', 'it came in 2014 but i never got to see it']\n",
      "[22, 1, 'hello happy friday ! hello there ! have you seen this movie ? what is the name of the movie ? maleficent with angelina jolie no i ve heard of it but never watched it what is it about ? it came in 2014 but i never got to see it fairy tale it s base don disney s sleeping beauty ', 'oh okay interesting']\n",
      "[11, 0, 'hi ', 'hello ! how are you doing today']\n",
      "[11, 0, 'hi hello ! how are you doing today ', 'i m doing great . almost friday']\n",
      "['cast kristen bell as anna the UNK year old princess of arendelle and elsa s younger sister livvy stubenrauch as 5 year old anna katie lopez as 5 year old anna singing UNK lee UNK as 9 year old anna idina menzel as elsa the 21 year old snow queen of arendelle and anna s elder sister eva bella as 8 year old elsa spencer lacey UNK as 12 year old elsa jonathan groff as kristoff an iceman who is accompanied by a reindeer named sven UNK UNK as 8 year old kristoff critical response the best animated musical to come out of disney since the tragic death of lyricist howard ashman whose work on the little mermaid and beauty and the beast helped build the studio s modern animated UNK into what it is today . while it lags the UNK bit on its way to the conclusion the script . . . really delivers it offers characters to care about along with some UNK twists and surprises along the way . you can practically see the broadway musical frozen is destined to become while watching disney s 3d animated princess tale . a great big snowy pleasure with an emotionally gripping core brilliant broadway style songs and a crafty plot . its first and third acts are better than the UNK middle but this is the rare example of a walt disney animation studios effort that reaches as deep as a pixar film . frozen is both a UNK of disney s UNK UNK UNK and a UNK of disney coming to terms with its own legacy and its own identity . it s also a just UNK terrific bit of family entertainment . wouldirector chris buck jennifer lee genre comedy adventure animation introduction frozen is a 2013 american 3d computer animated musical fantasy film produced by walt disney animation studios and released by walt disney pictures . it is the 53rd disney animated feature film . inspired by hans christian andersen s fairy tale the snow queen the film tells the story of a fearless princess who sets off on a journey alongside a rugged iceman his loyal pet reindeer and a UNK ve snowman to find her estranged sister whose icy powers have inadvertently trapped the kingdom in eternal winter . amoviename frozen rating rotten tomatoes 89 ametacritics 74 100 cinemascore a year 2013', 'princess elsa of arendelle UNK cryokinetic magic often using it to play with her younger sister anna . after elsa accidentally injures anna with her magic their parents the king and queen rush both siblings to a UNK of trolls led by grand pabbie . he heals anna but UNK her memories to UNK UNK of elsa s magic warning elsa that she must learn to control her powers . the king and queen UNK both sisters within the castle . elsa shuts out anna causing a UNK between them . elsa UNK her magic rather than UNK it causing her to become more insecure . when the sisters are teenagers their parents die at sea during a storm .', 'when elsa turns UNK one she is to be UNK queen of arendelle . she is terrified that the kingdom s UNK might find out about her powers and fear for her . the castle UNK open to the public and visiting dignitaries for the first time in years . UNK them is the UNK duke of UNK and the UNK prince hans of the southern UNK with whom anna falls head over UNK in love . elsa s UNK happens without a hitch but she still remains UNK from anna . when hans proposes to anna elsa UNK accidentally UNK her powers before the court . the duke UNK her a monster . elsa flees the kingdom but her UNK magic UNK arendelle in an eternal winter . reaching the north mountain elsa UNK her crown and creates a palace of ice in which to live a UNK life .', 'reaching the ice palace anna meets elsa but when she reveals what has become of arendelle elsa becomes UNK and accidentally freezes anna s heart . she then summons a giant snow creature named marshmallow who chases anna kristoff and olaf away . anna s hair begins turning white so kristoff takes her to meet the trolls his adoptive family . grand pabbie reveals that anna will freeze solid unless an act of true love UNK the spell . kristoff races anna back home so hans can give her true love s kiss . hans and his men reach elsa s palace defeating marshmallow and UNK elsa . anna is delivered to hans but rather than kissing her he instead reveals that he has actually been UNK to UNK the UNK of arendelle by UNK both sisters . he locks anna in a room to die and manipulates the dignitaries into believing that elsa killed her . he orders the queen s UNK only to discover she has escaped her UNK UNK .']\n"
     ]
    }
   ],
   "source": [
    "wiki_path = '../WikiData'\n",
    "\n",
    "corpus_name = 'seq+att'\n",
    "conv_path = os.path.join('../Conversations',corpus_name)\n",
    "dataCor = Corpus(conv_path,wiki_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "Iteration: 100; Percent complete: 0.3%; Average loss: 6.2517\n",
      "Iteration: 200; Percent complete: 0.7%; Average loss: 5.4831\n",
      "Iteration: 300; Percent complete: 1.0%; Average loss: 5.3722\n",
      "Iteration: 400; Percent complete: 1.3%; Average loss: 5.3216\n",
      "Iteration: 500; Percent complete: 1.7%; Average loss: 5.2460\n",
      "Iteration: 600; Percent complete: 2.0%; Average loss: 5.2104\n",
      "Iteration: 700; Percent complete: 2.3%; Average loss: 5.1505\n",
      "Iteration: 800; Percent complete: 2.7%; Average loss: 5.0602\n",
      "Iteration: 900; Percent complete: 3.0%; Average loss: 5.0556\n",
      "Iteration: 1000; Percent complete: 3.3%; Average loss: 4.9946\n",
      "Iteration: 1100; Percent complete: 3.7%; Average loss: 4.8683\n",
      "Iteration: 1200; Percent complete: 4.0%; Average loss: 4.8578\n",
      "Iteration: 1300; Percent complete: 4.3%; Average loss: 4.8109\n",
      "Iteration: 1400; Percent complete: 4.7%; Average loss: 4.8205\n",
      "Iteration: 1500; Percent complete: 5.0%; Average loss: 4.8487\n",
      "Iteration: 1600; Percent complete: 5.3%; Average loss: 4.7468\n",
      "Iteration: 1700; Percent complete: 5.7%; Average loss: 4.6239\n",
      "Iteration: 1800; Percent complete: 6.0%; Average loss: 4.7270\n",
      "Iteration: 1900; Percent complete: 6.3%; Average loss: 4.6693\n",
      "Iteration: 2000; Percent complete: 6.7%; Average loss: 4.5414\n",
      "Iteration: 2100; Percent complete: 7.0%; Average loss: 4.5267\n",
      "Iteration: 2200; Percent complete: 7.3%; Average loss: 4.5937\n",
      "Iteration: 2300; Percent complete: 7.7%; Average loss: 4.5679\n",
      "Iteration: 2400; Percent complete: 8.0%; Average loss: 4.4976\n",
      "Iteration: 2500; Percent complete: 8.3%; Average loss: 4.4446\n",
      "Iteration: 2600; Percent complete: 8.7%; Average loss: 4.4353\n",
      "Iteration: 2700; Percent complete: 9.0%; Average loss: 4.4113\n",
      "Iteration: 2800; Percent complete: 9.3%; Average loss: 4.2992\n",
      "Iteration: 2900; Percent complete: 9.7%; Average loss: 4.4068\n",
      "Iteration: 3000; Percent complete: 10.0%; Average loss: 4.3203\n",
      "Iteration: 3100; Percent complete: 10.3%; Average loss: 4.4059\n",
      "Iteration: 3200; Percent complete: 10.7%; Average loss: 4.3328\n",
      "Iteration: 3300; Percent complete: 11.0%; Average loss: 4.3288\n",
      "Iteration: 3400; Percent complete: 11.3%; Average loss: 4.2994\n",
      "Iteration: 3500; Percent complete: 11.7%; Average loss: 4.3725\n",
      "Iteration: 3600; Percent complete: 12.0%; Average loss: 4.3641\n",
      "Iteration: 3700; Percent complete: 12.3%; Average loss: 4.2421\n",
      "Iteration: 3800; Percent complete: 12.7%; Average loss: 4.2318\n",
      "Iteration: 3900; Percent complete: 13.0%; Average loss: 4.2157\n",
      "Iteration: 4000; Percent complete: 13.3%; Average loss: 4.3143\n",
      "Iteration: 4100; Percent complete: 13.7%; Average loss: 4.1999\n",
      "Iteration: 4200; Percent complete: 14.0%; Average loss: 4.2002\n",
      "Iteration: 4300; Percent complete: 14.3%; Average loss: 4.1030\n",
      "Iteration: 4400; Percent complete: 14.7%; Average loss: 4.2542\n",
      "Iteration: 4500; Percent complete: 15.0%; Average loss: 4.0708\n",
      "Iteration: 4600; Percent complete: 15.3%; Average loss: 4.1794\n",
      "Iteration: 4700; Percent complete: 15.7%; Average loss: 4.1570\n",
      "Iteration: 4800; Percent complete: 16.0%; Average loss: 4.0715\n",
      "Iteration: 4900; Percent complete: 16.3%; Average loss: 3.9939\n",
      "Iteration: 5000; Percent complete: 16.7%; Average loss: 4.0725\n",
      "Iteration: 5100; Percent complete: 17.0%; Average loss: 4.0039\n",
      "Iteration: 5200; Percent complete: 17.3%; Average loss: 4.0927\n",
      "Iteration: 5300; Percent complete: 17.7%; Average loss: 4.1103\n",
      "Iteration: 5400; Percent complete: 18.0%; Average loss: 4.1097\n",
      "Iteration: 5500; Percent complete: 18.3%; Average loss: 3.8815\n",
      "Iteration: 5600; Percent complete: 18.7%; Average loss: 4.0317\n",
      "Iteration: 5700; Percent complete: 19.0%; Average loss: 3.9934\n",
      "Iteration: 5800; Percent complete: 19.3%; Average loss: 3.9332\n",
      "Iteration: 5900; Percent complete: 19.7%; Average loss: 3.8985\n",
      "Iteration: 6000; Percent complete: 20.0%; Average loss: 3.9166\n",
      "Iteration: 6100; Percent complete: 20.3%; Average loss: 3.9865\n",
      "Iteration: 6200; Percent complete: 20.7%; Average loss: 3.9524\n",
      "Iteration: 6300; Percent complete: 21.0%; Average loss: 3.9510\n",
      "Iteration: 6400; Percent complete: 21.3%; Average loss: 3.9875\n",
      "Iteration: 6500; Percent complete: 21.7%; Average loss: 3.9798\n",
      "Iteration: 6600; Percent complete: 22.0%; Average loss: 3.9125\n",
      "Iteration: 6700; Percent complete: 22.3%; Average loss: 3.7225\n",
      "Iteration: 6800; Percent complete: 22.7%; Average loss: 3.8837\n",
      "Iteration: 6900; Percent complete: 23.0%; Average loss: 3.8035\n",
      "Iteration: 7000; Percent complete: 23.3%; Average loss: 3.9969\n",
      "Iteration: 7100; Percent complete: 23.7%; Average loss: 3.9863\n",
      "Iteration: 7200; Percent complete: 24.0%; Average loss: 3.8635\n",
      "Iteration: 7300; Percent complete: 24.3%; Average loss: 3.7333\n",
      "Iteration: 7400; Percent complete: 24.7%; Average loss: 3.8926\n",
      "Iteration: 7500; Percent complete: 25.0%; Average loss: 3.8285\n",
      "Iteration: 7600; Percent complete: 25.3%; Average loss: 3.8665\n",
      "Iteration: 7700; Percent complete: 25.7%; Average loss: 3.8974\n",
      "Iteration: 7800; Percent complete: 26.0%; Average loss: 3.7998\n",
      "Iteration: 7900; Percent complete: 26.3%; Average loss: 3.7924\n",
      "Iteration: 8000; Percent complete: 26.7%; Average loss: 3.7407\n",
      "Iteration: 8100; Percent complete: 27.0%; Average loss: 3.8334\n",
      "Iteration: 8200; Percent complete: 27.3%; Average loss: 3.7817\n",
      "Iteration: 8300; Percent complete: 27.7%; Average loss: 3.7191\n",
      "Iteration: 8400; Percent complete: 28.0%; Average loss: 3.7059\n",
      "Iteration: 8500; Percent complete: 28.3%; Average loss: 3.8677\n",
      "Iteration: 8600; Percent complete: 28.7%; Average loss: 3.7804\n",
      "Iteration: 8700; Percent complete: 29.0%; Average loss: 3.7416\n",
      "Iteration: 8800; Percent complete: 29.3%; Average loss: 3.7273\n",
      "Iteration: 8900; Percent complete: 29.7%; Average loss: 3.8637\n",
      "Iteration: 9000; Percent complete: 30.0%; Average loss: 3.8758\n",
      "Iteration: 9100; Percent complete: 30.3%; Average loss: 3.6450\n",
      "Iteration: 9200; Percent complete: 30.7%; Average loss: 3.7965\n",
      "Iteration: 9300; Percent complete: 31.0%; Average loss: 3.8219\n",
      "Iteration: 9400; Percent complete: 31.3%; Average loss: 3.5959\n",
      "Iteration: 9500; Percent complete: 31.7%; Average loss: 3.6965\n",
      "Iteration: 9600; Percent complete: 32.0%; Average loss: 3.6960\n",
      "Iteration: 9700; Percent complete: 32.3%; Average loss: 3.4478\n",
      "Iteration: 9800; Percent complete: 32.7%; Average loss: 3.6251\n",
      "Iteration: 9900; Percent complete: 33.0%; Average loss: 3.6251\n",
      "Iteration: 10000; Percent complete: 33.3%; Average loss: 3.6981\n",
      "Iteration: 10100; Percent complete: 33.7%; Average loss: 3.6433\n",
      "Iteration: 10200; Percent complete: 34.0%; Average loss: 3.7105\n",
      "Iteration: 10300; Percent complete: 34.3%; Average loss: 3.3817\n",
      "Iteration: 10400; Percent complete: 34.7%; Average loss: 3.6680\n",
      "Iteration: 10500; Percent complete: 35.0%; Average loss: 3.5641\n",
      "Iteration: 10600; Percent complete: 35.3%; Average loss: 3.5739\n",
      "Iteration: 10700; Percent complete: 35.7%; Average loss: 3.7300\n",
      "Iteration: 10800; Percent complete: 36.0%; Average loss: 3.6458\n",
      "Iteration: 10900; Percent complete: 36.3%; Average loss: 3.5374\n",
      "Iteration: 11000; Percent complete: 36.7%; Average loss: 3.6160\n",
      "Iteration: 11100; Percent complete: 37.0%; Average loss: 3.6180\n",
      "Iteration: 11200; Percent complete: 37.3%; Average loss: 3.7615\n",
      "Iteration: 11300; Percent complete: 37.7%; Average loss: 3.4607\n",
      "Iteration: 11400; Percent complete: 38.0%; Average loss: 3.7267\n",
      "Iteration: 11500; Percent complete: 38.3%; Average loss: 3.5135\n",
      "Iteration: 11600; Percent complete: 38.7%; Average loss: 3.6158\n",
      "Iteration: 11700; Percent complete: 39.0%; Average loss: 3.5568\n",
      "Iteration: 11800; Percent complete: 39.3%; Average loss: 3.5675\n",
      "Iteration: 11900; Percent complete: 39.7%; Average loss: 3.6835\n",
      "Iteration: 12000; Percent complete: 40.0%; Average loss: 3.5740\n",
      "Iteration: 12100; Percent complete: 40.3%; Average loss: 3.6559\n",
      "Iteration: 12200; Percent complete: 40.7%; Average loss: 3.5524\n",
      "Iteration: 12300; Percent complete: 41.0%; Average loss: 3.4739\n",
      "Iteration: 12400; Percent complete: 41.3%; Average loss: 3.4554\n",
      "Iteration: 12500; Percent complete: 41.7%; Average loss: 3.5751\n",
      "Iteration: 12600; Percent complete: 42.0%; Average loss: 3.4020\n",
      "Iteration: 12700; Percent complete: 42.3%; Average loss: 3.4851\n",
      "Iteration: 12800; Percent complete: 42.7%; Average loss: 3.3254\n",
      "Iteration: 12900; Percent complete: 43.0%; Average loss: 3.4922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13000; Percent complete: 43.3%; Average loss: 3.5369\n",
      "Iteration: 13100; Percent complete: 43.7%; Average loss: 3.5020\n",
      "Iteration: 13200; Percent complete: 44.0%; Average loss: 3.5995\n",
      "Iteration: 13300; Percent complete: 44.3%; Average loss: 3.5778\n",
      "Iteration: 13400; Percent complete: 44.7%; Average loss: 3.4643\n",
      "Iteration: 13500; Percent complete: 45.0%; Average loss: 3.4766\n",
      "Iteration: 13600; Percent complete: 45.3%; Average loss: 3.4846\n",
      "Iteration: 13700; Percent complete: 45.7%; Average loss: 3.3843\n",
      "Iteration: 13800; Percent complete: 46.0%; Average loss: 3.4807\n",
      "Iteration: 13900; Percent complete: 46.3%; Average loss: 3.4453\n",
      "Iteration: 14000; Percent complete: 46.7%; Average loss: 3.3627\n",
      "Iteration: 14100; Percent complete: 47.0%; Average loss: 3.6278\n",
      "Iteration: 14200; Percent complete: 47.3%; Average loss: 3.2918\n",
      "Iteration: 14300; Percent complete: 47.7%; Average loss: 3.3650\n",
      "Iteration: 14400; Percent complete: 48.0%; Average loss: 3.3241\n",
      "Iteration: 14500; Percent complete: 48.3%; Average loss: 3.4200\n",
      "Iteration: 14600; Percent complete: 48.7%; Average loss: 3.5346\n",
      "Iteration: 14700; Percent complete: 49.0%; Average loss: 3.3249\n",
      "Iteration: 14800; Percent complete: 49.3%; Average loss: 3.4587\n",
      "Iteration: 14900; Percent complete: 49.7%; Average loss: 3.4075\n",
      "Iteration: 15000; Percent complete: 50.0%; Average loss: 3.2638\n",
      "Iteration: 15100; Percent complete: 50.3%; Average loss: 3.3180\n",
      "Iteration: 15200; Percent complete: 50.7%; Average loss: 3.5542\n",
      "Iteration: 15300; Percent complete: 51.0%; Average loss: 3.3525\n",
      "Iteration: 15400; Percent complete: 51.3%; Average loss: 3.4260\n",
      "Iteration: 15500; Percent complete: 51.7%; Average loss: 3.3747\n",
      "Iteration: 15600; Percent complete: 52.0%; Average loss: 3.4084\n",
      "Iteration: 15700; Percent complete: 52.3%; Average loss: 3.3176\n",
      "Iteration: 15800; Percent complete: 52.7%; Average loss: 3.2619\n",
      "Iteration: 15900; Percent complete: 53.0%; Average loss: 3.4584\n",
      "Iteration: 16000; Percent complete: 53.3%; Average loss: 3.3886\n",
      "Iteration: 16100; Percent complete: 53.7%; Average loss: 3.3634\n",
      "Iteration: 16200; Percent complete: 54.0%; Average loss: 3.3343\n",
      "Iteration: 16300; Percent complete: 54.3%; Average loss: 3.5361\n",
      "Iteration: 16400; Percent complete: 54.7%; Average loss: 3.2202\n",
      "Iteration: 16500; Percent complete: 55.0%; Average loss: 3.2352\n",
      "Iteration: 16600; Percent complete: 55.3%; Average loss: 3.3931\n",
      "Iteration: 16700; Percent complete: 55.7%; Average loss: 3.3402\n",
      "Iteration: 16800; Percent complete: 56.0%; Average loss: 3.1113\n",
      "Iteration: 16900; Percent complete: 56.3%; Average loss: 3.3423\n",
      "Iteration: 17000; Percent complete: 56.7%; Average loss: 3.1886\n",
      "Iteration: 17100; Percent complete: 57.0%; Average loss: 3.3981\n",
      "Iteration: 17200; Percent complete: 57.3%; Average loss: 3.3099\n",
      "Iteration: 17300; Percent complete: 57.7%; Average loss: 3.1920\n",
      "Iteration: 17400; Percent complete: 58.0%; Average loss: 3.2363\n",
      "Iteration: 17500; Percent complete: 58.3%; Average loss: 3.3535\n",
      "Iteration: 17600; Percent complete: 58.7%; Average loss: 3.1194\n",
      "Iteration: 17700; Percent complete: 59.0%; Average loss: 3.3360\n",
      "Iteration: 17800; Percent complete: 59.3%; Average loss: 3.4080\n",
      "Iteration: 17900; Percent complete: 59.7%; Average loss: 3.3797\n",
      "Iteration: 18000; Percent complete: 60.0%; Average loss: 3.3147\n",
      "Iteration: 18100; Percent complete: 60.3%; Average loss: 3.2174\n",
      "Iteration: 18200; Percent complete: 60.7%; Average loss: 3.1596\n",
      "Iteration: 18300; Percent complete: 61.0%; Average loss: 3.3330\n",
      "Iteration: 18400; Percent complete: 61.3%; Average loss: 3.4398\n",
      "Iteration: 18500; Percent complete: 61.7%; Average loss: 3.2004\n",
      "Iteration: 18600; Percent complete: 62.0%; Average loss: 3.2882\n",
      "Iteration: 18700; Percent complete: 62.3%; Average loss: 3.4982\n",
      "Iteration: 18800; Percent complete: 62.7%; Average loss: 3.1830\n",
      "Iteration: 18900; Percent complete: 63.0%; Average loss: 3.1454\n",
      "Iteration: 19000; Percent complete: 63.3%; Average loss: 3.1333\n",
      "Iteration: 19100; Percent complete: 63.7%; Average loss: 3.0458\n",
      "Iteration: 19200; Percent complete: 64.0%; Average loss: 3.0601\n",
      "Iteration: 19300; Percent complete: 64.3%; Average loss: 3.2764\n",
      "Iteration: 19400; Percent complete: 64.7%; Average loss: 3.1517\n",
      "Iteration: 19500; Percent complete: 65.0%; Average loss: 3.4848\n",
      "Iteration: 19600; Percent complete: 65.3%; Average loss: 3.2949\n",
      "Iteration: 19700; Percent complete: 65.7%; Average loss: 3.1586\n",
      "Iteration: 19800; Percent complete: 66.0%; Average loss: 3.1755\n",
      "Iteration: 19900; Percent complete: 66.3%; Average loss: 3.1655\n",
      "Iteration: 20000; Percent complete: 66.7%; Average loss: 3.2710\n",
      "Iteration: 20100; Percent complete: 67.0%; Average loss: 3.2300\n",
      "Iteration: 20200; Percent complete: 67.3%; Average loss: 3.2332\n",
      "Iteration: 20300; Percent complete: 67.7%; Average loss: 3.0935\n",
      "Iteration: 20400; Percent complete: 68.0%; Average loss: 3.0820\n",
      "Iteration: 20500; Percent complete: 68.3%; Average loss: 3.0166\n",
      "Iteration: 20600; Percent complete: 68.7%; Average loss: 3.1562\n",
      "Iteration: 20700; Percent complete: 69.0%; Average loss: 2.9629\n",
      "Iteration: 20800; Percent complete: 69.3%; Average loss: 2.9151\n",
      "Iteration: 20900; Percent complete: 69.7%; Average loss: 2.9678\n",
      "Iteration: 21000; Percent complete: 70.0%; Average loss: 3.3777\n",
      "Iteration: 21100; Percent complete: 70.3%; Average loss: 3.2386\n",
      "Iteration: 21200; Percent complete: 70.7%; Average loss: 3.0785\n",
      "Iteration: 21300; Percent complete: 71.0%; Average loss: 3.1064\n",
      "Iteration: 21400; Percent complete: 71.3%; Average loss: 3.1456\n",
      "Iteration: 21500; Percent complete: 71.7%; Average loss: 3.1543\n",
      "Iteration: 21600; Percent complete: 72.0%; Average loss: 3.0772\n",
      "Iteration: 21700; Percent complete: 72.3%; Average loss: 3.2663\n",
      "Iteration: 21800; Percent complete: 72.7%; Average loss: 3.1686\n",
      "Iteration: 21900; Percent complete: 73.0%; Average loss: 2.9724\n",
      "Iteration: 22000; Percent complete: 73.3%; Average loss: 2.8307\n",
      "Iteration: 22100; Percent complete: 73.7%; Average loss: 3.3837\n",
      "Iteration: 22200; Percent complete: 74.0%; Average loss: 3.1370\n",
      "Iteration: 22300; Percent complete: 74.3%; Average loss: 2.9873\n",
      "Iteration: 22400; Percent complete: 74.7%; Average loss: 3.0231\n",
      "Iteration: 22500; Percent complete: 75.0%; Average loss: 2.9415\n",
      "Iteration: 22600; Percent complete: 75.3%; Average loss: 3.1391\n",
      "Iteration: 22700; Percent complete: 75.7%; Average loss: 2.9277\n",
      "Iteration: 22800; Percent complete: 76.0%; Average loss: 3.1178\n",
      "Iteration: 22900; Percent complete: 76.3%; Average loss: 3.2211\n",
      "Iteration: 23000; Percent complete: 76.7%; Average loss: 3.0556\n",
      "Iteration: 23100; Percent complete: 77.0%; Average loss: 3.0733\n",
      "Iteration: 23200; Percent complete: 77.3%; Average loss: 3.1278\n",
      "Iteration: 23300; Percent complete: 77.7%; Average loss: 2.9414\n",
      "Iteration: 23400; Percent complete: 78.0%; Average loss: 3.0606\n",
      "Iteration: 23500; Percent complete: 78.3%; Average loss: 3.0163\n",
      "Iteration: 23600; Percent complete: 78.7%; Average loss: 2.8353\n",
      "Iteration: 23700; Percent complete: 79.0%; Average loss: 3.1455\n",
      "Iteration: 23800; Percent complete: 79.3%; Average loss: 3.2115\n",
      "Iteration: 23900; Percent complete: 79.7%; Average loss: 3.2581\n",
      "Iteration: 24000; Percent complete: 80.0%; Average loss: 3.1305\n",
      "Iteration: 24100; Percent complete: 80.3%; Average loss: 3.0243\n",
      "Iteration: 24200; Percent complete: 80.7%; Average loss: 2.9847\n",
      "Iteration: 24300; Percent complete: 81.0%; Average loss: 3.0355\n",
      "Iteration: 24400; Percent complete: 81.3%; Average loss: 3.0256\n",
      "Iteration: 24500; Percent complete: 81.7%; Average loss: 3.0579\n",
      "Iteration: 24600; Percent complete: 82.0%; Average loss: 3.1071\n",
      "Iteration: 24700; Percent complete: 82.3%; Average loss: 2.9502\n",
      "Iteration: 24800; Percent complete: 82.7%; Average loss: 3.0782\n",
      "Iteration: 24900; Percent complete: 83.0%; Average loss: 2.9014\n",
      "Iteration: 25000; Percent complete: 83.3%; Average loss: 3.1018\n",
      "Iteration: 25100; Percent complete: 83.7%; Average loss: 3.0900\n",
      "Iteration: 25200; Percent complete: 84.0%; Average loss: 2.9283\n",
      "Iteration: 25300; Percent complete: 84.3%; Average loss: 3.0385\n",
      "Iteration: 25400; Percent complete: 84.7%; Average loss: 2.8552\n",
      "Iteration: 25500; Percent complete: 85.0%; Average loss: 3.1549\n",
      "Iteration: 25600; Percent complete: 85.3%; Average loss: 2.7814\n",
      "Iteration: 25700; Percent complete: 85.7%; Average loss: 3.0194\n",
      "Iteration: 25800; Percent complete: 86.0%; Average loss: 3.2064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 25900; Percent complete: 86.3%; Average loss: 2.9932\n",
      "Iteration: 26000; Percent complete: 86.7%; Average loss: 2.9506\n",
      "Iteration: 26100; Percent complete: 87.0%; Average loss: 2.8324\n",
      "Iteration: 26200; Percent complete: 87.3%; Average loss: 2.6330\n",
      "Iteration: 26300; Percent complete: 87.7%; Average loss: 3.0212\n",
      "Iteration: 26400; Percent complete: 88.0%; Average loss: 2.9558\n",
      "Iteration: 26500; Percent complete: 88.3%; Average loss: 2.9124\n",
      "Iteration: 26600; Percent complete: 88.7%; Average loss: 3.0091\n",
      "Iteration: 26700; Percent complete: 89.0%; Average loss: 2.7433\n",
      "Iteration: 26800; Percent complete: 89.3%; Average loss: 2.9448\n",
      "Iteration: 26900; Percent complete: 89.7%; Average loss: 2.9430\n",
      "Iteration: 27000; Percent complete: 90.0%; Average loss: 3.1407\n",
      "Iteration: 27100; Percent complete: 90.3%; Average loss: 3.0311\n",
      "Iteration: 27200; Percent complete: 90.7%; Average loss: 2.9282\n",
      "Iteration: 27300; Percent complete: 91.0%; Average loss: 3.0451\n",
      "Iteration: 27400; Percent complete: 91.3%; Average loss: 2.8345\n",
      "Iteration: 27500; Percent complete: 91.7%; Average loss: 2.6964\n",
      "Iteration: 27600; Percent complete: 92.0%; Average loss: 3.0228\n",
      "Iteration: 27700; Percent complete: 92.3%; Average loss: 2.7551\n",
      "Iteration: 27800; Percent complete: 92.7%; Average loss: 2.7798\n",
      "Iteration: 27900; Percent complete: 93.0%; Average loss: 3.0508\n",
      "Iteration: 28000; Percent complete: 93.3%; Average loss: 2.8948\n",
      "Iteration: 28100; Percent complete: 93.7%; Average loss: 2.8469\n",
      "Iteration: 28200; Percent complete: 94.0%; Average loss: 2.9157\n",
      "Iteration: 28300; Percent complete: 94.3%; Average loss: 2.9889\n",
      "Iteration: 28400; Percent complete: 94.7%; Average loss: 2.8024\n",
      "Iteration: 28500; Percent complete: 95.0%; Average loss: 2.7648\n",
      "Iteration: 28600; Percent complete: 95.3%; Average loss: 2.9666\n",
      "Iteration: 28700; Percent complete: 95.7%; Average loss: 2.8464\n",
      "Iteration: 28800; Percent complete: 96.0%; Average loss: 2.9668\n",
      "Iteration: 28900; Percent complete: 96.3%; Average loss: 2.9433\n",
      "Iteration: 29000; Percent complete: 96.7%; Average loss: 2.9267\n",
      "Iteration: 29100; Percent complete: 97.0%; Average loss: 3.1059\n",
      "Iteration: 29200; Percent complete: 97.3%; Average loss: 2.9454\n",
      "Iteration: 29300; Percent complete: 97.7%; Average loss: 2.8971\n",
      "Iteration: 29400; Percent complete: 98.0%; Average loss: 2.8923\n",
      "Iteration: 29500; Percent complete: 98.3%; Average loss: 2.9844\n",
      "Iteration: 29600; Percent complete: 98.7%; Average loss: 2.8814\n",
      "Iteration: 29700; Percent complete: 99.0%; Average loss: 2.7847\n",
      "Iteration: 29800; Percent complete: 99.3%; Average loss: 2.8776\n",
      "Iteration: 29900; Percent complete: 99.7%; Average loss: 2.8929\n",
      "Iteration: 30000; Percent complete: 100.0%; Average loss: 2.8329\n",
      "Building optimizers ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LuongAttnDecoderRNN(\n",
       "  (embedding): Embedding(7679, 100)\n",
       "  (embedding_dropout): Dropout(p=0.3)\n",
       "  (gru): GRU(400, 300)\n",
       "  (concat): Linear(in_features=600, out_features=300, bias=True)\n",
       "  (concatsec): Linear(in_features=1200, out_features=300, bias=True)\n",
       "  (out): Linear(in_features=300, out_features=7679, bias=True)\n",
       "  (attn): Attn()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "# if config.loadFilename:\n",
    "#     # If loading on same machine the model was trained on\n",
    "#     checkpoint = torch.load(config.loadFilename)\n",
    "#     # If loading a model trained on GPU to CPU\n",
    "#     #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "#     encoder_sd = checkpoint['en']\n",
    "#     sec_encoder_sd = checkpoint['sec_en']\n",
    "#     decoder_sd = checkpoint['de']\n",
    "#     encoder_optimizer_sd = checkpoint['en_opt']\n",
    "#     sec_encoder_optimizer_sd = checkpoint['sec_en_opt']\n",
    "#     decoder_optimizer_sd = checkpoint['de_opt']\n",
    "#     embedding_sd = checkpoint['embedding']\n",
    "#     voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, config.embedding_size)\n",
    "# if config.loadFilename:\n",
    "#     embedding.load_state_dict(embedding_sd)\n",
    "\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(config.embedding_size,config.hidden_size, embedding, config.encoder_n_layers, config.dropout)\n",
    "sec_encoder = EncoderRNN(config.embedding_size,config.hidden_size, embedding, config.encoder_n_layers, config.dropout)\n",
    "decoder = LuongAttnDecoderRNN(config.attn_model, embedding,config.embedding_size, config.encoder_n_layers,config.hidden_size, voc.num_words, config.decoder_n_layers, config.dropout)\n",
    "\n",
    "# if config.loadFilename:\n",
    "#     encoder.load_state_dict(encoder_sd)\n",
    "#     sec_encoder.load_state_dict(sec_encoder_sd)\n",
    "#     decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(config.device)\n",
    "sec_encoder = sec_encoder.to(config.device)\n",
    "decoder = decoder.to(config.device)\n",
    "print('Models built and ready to go!')\n",
    "\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "sec_encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=config.learning_rate)\n",
    "sec_encoder_optimizer = optim.Adam(sec_encoder.parameters(), lr=config.learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=config.learning_rate * config.decoder_learning_ratio)\n",
    "# if config.loadFilename:\n",
    "#     encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "#     sec_encoder_optimizer.load_state_dict(sec_encoder_optimizer_sd)\n",
    "#     decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainIters(voc, pairs, wiki_strings, encoder, sec_encoder,decoder, encoder_optimizer,sec_encoder_optimizer,decoder_optimizer,embedding,save_dir)\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=config.learning_rate)\n",
    "sec_encoder_optimizer = optim.Adam(sec_encoder.parameters(), lr=config.learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=config.learning_rate * config.decoder_learning_ratio)\n",
    "# if config.loadFilename:\n",
    "#     encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "#     sec_encoder_optimizer.load_state_dict(sec_encoder_optimizer_sd)\n",
    "#     decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "sec_encoder.eval()\n",
    "decoder.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateInput(encoder, sec_encoder, decoder, searcher, voc,wiki_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have you seen this movie ?\n",
      "have you seen this movie ?\n",
      "i am great ! how are you ?\n",
      "i am good how are you ?\n",
      "i thought it was a very underrated movie . i thought it was very good .\n",
      "i think it was a very underrated movie . i think the critic scores given were very low .\n",
      "no i have not .\n",
      "i have not . what is it about ?\n",
      "it is a biographical movie about leonardo dicaprio who are basically in their dream world where they can bend reality\n",
      "it is a really trippy movie with leonardo dicaprio . they are basically in a dream world where they can bend reality\n",
      "can you tell me the name of the movie ?\n",
      "can you tell me more about the movie ?\n",
      "no i have not\n",
      "no i have not\n",
      "it has a rotten tomatoes score of 6 . 8 10 . 8 10 .\n",
      "i highly recommend it it has a rotten tomatoes score of 6 . 8 10 .\n",
      "it s about the antics of jordan belfort .\n",
      "it s about the antics of jordan belfort .\n",
      "who is the main actor ?\n",
      "who is the main character\n",
      "what did you think about the rotten tomatoes score ?\n",
      "it was ! what did you think about the rotten tomatoes review of 84 ?\n",
      "no i do not really recommend captain america .\n",
      "no i do not .\n",
      "i thought angelina jolie was good in this movie .\n",
      "i thought angelina jolie was great in this movie .\n",
      "it came out in 2010 . it came out in 2010 years ago already .\n",
      "right right . it came out 2010 . cant believe 2010 is 8 years ago already .\n",
      "it is about a con man who successfully cons millions of dollars by posing in corruption\n",
      "its about a con man who successfully cons millions of dollars by posing as an airline pilot\n",
      "have you seen despicable me ?\n",
      "have you seen despicable me ?\n",
      "it was released in 2012 and it was released in 2012 .\n",
      "haha . it was released in 2012 and it is a superhero movie .\n",
      "no i have not seen it . assuming it came out in 2017 .\n",
      "no i have not seen it\n",
      "have you seen this movie ?\n",
      "have you seen this movie ?\n",
      "sounds like a good movie\n",
      "sounds like a good story\n",
      "no i have not\n",
      "no i have not seen it\n",
      "robert downey jr . robert downey jr . s portrayal of tony stark was incredible .\n",
      "i do too . i find that robert downey jr . s portrayal of tony stark is incredible .\n",
      "razzle dazzle technique and tom hanks\n",
      "razzle dazzle technique and unusual look\n",
      "i did not know that it was based on non fiction .\n",
      "i did not . i would have never guessed it was based on non fiction .\n",
      "the movie is robert downey jr . chris evans and chris evans scarlett johannson jeremy renner and tom hiddleston .\n",
      "the movie stars robert downey jr . chris evans mark ruffalo chris hemsworth scarlett johannson jeremy renner and tom hiddleston .\n",
      "it is about a boy who is mistakenly left behind when his family goes on vaccation to paris for chrismas\n",
      "it is about a boy who is mistakenly left behind when his family goes on vaccation to paris for chrismas\n",
      "who is the main character ?\n",
      "who is the main star of this movie ?\n",
      "what is the movie about ?\n",
      "what is the movie about ?\n",
      "i thought it was based on rotten tomatoes but i thought it was pretty good . how about you ?\n",
      "the ratings for the movie were not so great with a 48 on rotten tomatoes but i thought it was pretty good .\n",
      "it s based on the booked and talks about the life of jay gatsby dicaprio during the 1920s of the 1920s .\n",
      "anyways the movie is based on the booked and talks about the life of jay gatsby dicaprio during the 1920s .\n",
      "what is the name of the movie ?\n",
      "what is the name of the movie ?\n",
      "i thought it was great .\n",
      "i thought it was pretty good .\n",
      "what was your favorite part of the movie ?\n",
      "what was your favorite part of the movie ?\n",
      "robert downey jr . chris evans .\n",
      "robert downey jr . as iron man\n",
      "i do think i ll have to give it a try to give it a try to give it a try\n",
      "it has a lot of good actors in it so i think i might enjoy it i ll have to give it a try\n",
      "what is it about ?\n",
      "oh cool what is it about ?\n",
      "i mean rachel mcadams as the mega bitch\n",
      "i just remember it having rachel mcadams as the mega bitch\n",
      "what was your favorite scene ?\n",
      "what was your favorite character ?\n",
      "it was launched in 2013\n",
      "it was launched in 2013\n",
      "what is it about ?\n",
      "what is it about ?\n",
      "the wolf of wall street\n",
      "the wolf of wall street\n",
      "0.03576136620121998\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from data_utils import normalizeString,indexesFromSentence\n",
    "\n",
    "# def batch_bleu(ref_batch, candi_batch):\n",
    "#     zip_batch = zip(ref_batch, candi_batch)\n",
    "#     scores = [sentence_bleu([ref.split()],cand.split()) for ref,cand in zip_batch]\n",
    "#     avg_score = sum(scores)/len(scores)\n",
    "#     return avg_score\n",
    "\n",
    "def evaluate(encoder, sec_encoder, decoder, searcher, voc, sentence, wikiSec):\n",
    "    max_length=config.MAX_LENGTH\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)] #(1,L)\n",
    "    sec_indexes = [indexesFromSentence(voc,wikiSec)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch]) #(1,)\n",
    "    sec_lengths = torch.tensor([len(indexes) for indexes in sec_indexes])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1) #(L,1)\n",
    "    sec_batch = torch.LongTensor(sec_indexes).transpose(0, 1) \n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(config.device)\n",
    "    sec_batch = sec_batch.to(config.device)\n",
    "    lengths = lengths.to(config.device)\n",
    "    sec_lengths = sec_lengths.to(config.device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, sec_batch, sec_lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [voc.index2word[token] for token in tokens if not (token == config.EOS_token or token == config.PAD_token)]\n",
    "    return decoded_words\n",
    "\n",
    "def batch_bleu(encoder,sec_encoder,decoder,searcher,voc, wiki_strings, dev_batch):\n",
    "    scores = []\n",
    "    for conv in dev_batch:\n",
    "        doc_idx = conv[0]\n",
    "        sec_idx = conv[1]\n",
    "        candidate_sentence = evaluate(encoder,sec_encoder,decoder,searcher,voc,conv[2],wiki_strings[doc_idx][sec_idx])\n",
    "#         print(candidate_sentence)\n",
    "        refs = conv[3].split()\n",
    "        s = sentence_bleu([refs],candidate_sentence)\n",
    "        if s > 0.3:\n",
    "            print(' '.join(candidate_sentence))\n",
    "            print(' '.join(refs))\n",
    "        scores.append(s)\n",
    "    return sum(scores)/len(scores)\n",
    "# Initialize search module\n",
    "# searcher = GreedySearchDecoder(encoder, sec_encoder, decoder)\n",
    "searcher = BeamSearchDecoder(encoder,sec_encoder,decoder)\n",
    "res = batch_bleu(encoder,sec_encoder,decoder,searcher,voc,wiki_strings,pairs[:1000])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
